{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKdXa6dsxcLP",
        "outputId": "6476741d-6c14-458f-9372-45bda3618f8d"
      },
      "source": [
        "# Import and restructure data\n",
        "\n",
        "import pandas as pd\n",
        "data = pd.read_csv('2020-12-31-DynamicallyGeneratedHateDataset-entries-v0.1.csv', error_bad_lines=False)\n",
        "text = data[['text']]\n",
        "text['index'] = text.index\n",
        "documents = text"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW_HGBBIyCnu",
        "outputId": "c3869e12-5a12-4ad8-88e2-bd1dbdd2d095"
      },
      "source": [
        "print(len(documents))\n",
        "print(documents[:5])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40623\n",
            "                                                text  index\n",
            "0  It should be legal for a husband to rape his wife      0\n",
            "1                            dalits are lowlife scum      1\n",
            "2                                dalits are lowlives      2\n",
            "3  It was a better world when women dare not ques...      3\n",
            "4  I don't work this hard so that those immigrant...      4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrhGeepNzDoV",
        "outputId": "76d144a6-ed2f-4ade-9f57-8e091c9a0be7"
      },
      "source": [
        "# Preprocess Data\n",
        "\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGLbAzRAzNIj"
      },
      "source": [
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urQjSY2_zV6n",
        "outputId": "fd9e4a8f-8f96-4519-c806-768b58505c0e"
      },
      "source": [
        "doc_sample = documents[documents['index'] == 4310].values[0][0]\n",
        "print('original document: ')\n",
        "words = []\n",
        "for word in doc_sample.split(' '):\n",
        "    words.append(word)\n",
        "print(words)\n",
        "print('\\n\\n tokenized and lemmatized document: ')\n",
        "print(preprocess(doc_sample))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original document: \n",
            "['My', 'black', 'watch', 'reminds', 'me', 'of', 'all', 'black', 'men']\n",
            "\n",
            "\n",
            " tokenized and lemmatized document: \n",
            "['black', 'watch', 'remind', 'black']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNvon0BXzgYc",
        "outputId": "056c87ec-49cb-4d1b-bd4d-36b958b641ca"
      },
      "source": [
        "processed_docs = documents['text'].map(preprocess)\n",
        "processed_docs[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0              [legal, husband, rape, wife]\n",
              "1                     [dalit, lowlif, scum]\n",
              "2                           [dalit, lowliv]\n",
              "3    [better, world, women, dare, question]\n",
              "4             [work, hard, immigr, benefit]\n",
              "5            [work, hard, migrant, benefit]\n",
              "6            [work, hard, foreign, benefit]\n",
              "7             [work, hard, hispan, benefit]\n",
              "8               [work, hard, chav, benefit]\n",
              "9               [work, hard, paki, benefit]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhTdwpP8JsMh",
        "outputId": "a961ab05-23d6-489c-e1f1-354320632149"
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 husband\n",
            "1 legal\n",
            "2 rape\n",
            "3 wife\n",
            "4 dalit\n",
            "5 lowlif\n",
            "6 scum\n",
            "7 lowliv\n",
            "8 better\n",
            "9 dare\n",
            "10 question\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy72_w3IJ4sn"
      },
      "source": [
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im4FcAyBKA_7",
        "outputId": "75a5b3ec-0b9f-4a96-d6e9-90dcbf82d5dd"
      },
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "bow_corpus[4310]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(29, 2), (318, 1), (1089, 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4kEmF_MKFeL",
        "outputId": "cbbf5dd0-4ac8-4080-aac8-1feb1aeb163d"
      },
      "source": [
        "bow_doc_4310 = bow_corpus[4310]\n",
        "for i in range(len(bow_doc_4310)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
        "                                               dictionary[bow_doc_4310[i][0]], \n",
        "bow_doc_4310[i][1]))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 29 (\"black\") appears 2 time.\n",
            "Word 318 (\"watch\") appears 1 time.\n",
            "Word 1089 (\"remind\") appears 1 time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFHXKTHEKOfb",
        "outputId": "fa92c01c-7a56-442a-859e-860470d9a5f1"
      },
      "source": [
        "from gensim import corpora, models\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "from pprint import pprint\n",
        "for doc in corpus_tfidf:\n",
        "    pprint(doc)\n",
        "    break"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.4816817283932777),\n",
            " (1, 0.5260359103822929),\n",
            " (2, 0.524052918770054),\n",
            " (3, 0.46544330680430235)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHznk1yuKY55"
      },
      "source": [
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZ43EHn2KnE5",
        "outputId": "0ca33334-deac-43c4-fefa-0ae0f4d0acfa"
      },
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.022*\"countri\" + 0.019*\"year\" + 0.019*\"peopl\" + 0.015*\"allow\" + 0.014*\"leav\" + 0.013*\"issu\" + 0.012*\"chang\" + 0.012*\"life\" + 0.011*\"shit\" + 0.011*\"like\"\n",
            "Topic: 1 \n",
            "Words: 0.058*\"need\" + 0.038*\"stop\" + 0.035*\"peopl\" + 0.032*\"like\" + 0.022*\"word\" + 0.021*\"think\" + 0.021*\"fuck\" + 0.017*\"feel\" + 0.016*\"have\" + 0.015*\"call\"\n",
            "Topic: 2 \n",
            "Words: 0.025*\"good\" + 0.021*\"time\" + 0.019*\"thing\" + 0.017*\"go\" + 0.016*\"peopl\" + 0.014*\"work\" + 0.012*\"children\" + 0.011*\"talk\" + 0.010*\"school\" + 0.010*\"societi\"\n",
            "Topic: 3 \n",
            "Words: 0.046*\"peopl\" + 0.034*\"think\" + 0.025*\"wrong\" + 0.024*\"jew\" + 0.023*\"make\" + 0.019*\"kill\" + 0.013*\"disabl\" + 0.012*\"kid\" + 0.010*\"turn\" + 0.010*\"understand\"\n",
            "Topic: 4 \n",
            "Words: 0.039*\"fuck\" + 0.037*\"opinion\" + 0.031*\"want\" + 0.024*\"come\" + 0.022*\"think\" + 0.015*\"express\" + 0.015*\"idiot\" + 0.014*\"beauti\" + 0.014*\"allow\" + 0.013*\"inform\"\n",
            "Topic: 5 \n",
            "Words: 0.064*\"like\" + 0.032*\"peopl\" + 0.028*\"black\" + 0.022*\"person\" + 0.020*\"hate\" + 0.018*\"actual\" + 0.013*\"world\" + 0.013*\"girl\" + 0.013*\"fuck\" + 0.011*\"talk\"\n",
            "Topic: 6 \n",
            "Words: 0.048*\"love\" + 0.045*\"peopl\" + 0.036*\"muslim\" + 0.029*\"fuck\" + 0.029*\"know\" + 0.023*\"hate\" + 0.018*\"tran\" + 0.018*\"marri\" + 0.016*\"get\" + 0.014*\"want\"\n",
            "Topic: 7 \n",
            "Words: 0.108*\"women\" + 0.035*\"fuck\" + 0.016*\"shit\" + 0.016*\"like\" + 0.016*\"know\" + 0.015*\"say\" + 0.010*\"point\" + 0.009*\"better\" + 0.009*\"look\" + 0.008*\"woman\"\n",
            "Topic: 8 \n",
            "Words: 0.104*\"black\" + 0.059*\"white\" + 0.046*\"peopl\" + 0.020*\"tell\" + 0.017*\"mean\" + 0.015*\"start\" + 0.014*\"think\" + 0.013*\"matter\" + 0.011*\"attack\" + 0.010*\"woman\"\n",
            "Topic: 9 \n",
            "Words: 0.025*\"peopl\" + 0.019*\"best\" + 0.018*\"friend\" + 0.018*\"famili\" + 0.014*\"look\" + 0.013*\"love\" + 0.013*\"dont\" + 0.012*\"explain\" + 0.012*\"fuck\" + 0.012*\"want\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-THkd1yKvdQ",
        "outputId": "aa16e1bf-dee8-4d9d-df06-ed651e8292ec"
      },
      "source": [
        "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
        "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
        "    print('Topic: {} Word: {}'.format(idx, topic))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 Word: 0.016*\"concept\" + 0.015*\"talk\" + 0.012*\"love\" + 0.011*\"like\" + 0.010*\"women\" + 0.010*\"peopl\" + 0.010*\"idiot\" + 0.010*\"opposit\" + 0.009*\"chines\" + 0.009*\"marriag\"\n",
            "Topic: 1 Word: 0.015*\"tran\" + 0.015*\"want\" + 0.013*\"peopl\" + 0.011*\"like\" + 0.010*\"fuck\" + 0.010*\"think\" + 0.009*\"inform\" + 0.008*\"evil\" + 0.008*\"know\" + 0.008*\"common\"\n",
            "Topic: 2 Word: 0.016*\"peopl\" + 0.012*\"black\" + 0.011*\"fuck\" + 0.011*\"say\" + 0.010*\"lie\" + 0.009*\"vile\" + 0.008*\"like\" + 0.008*\"disabl\" + 0.008*\"trust\" + 0.008*\"racist\"\n",
            "Topic: 3 Word: 0.026*\"fuck\" + 0.016*\"shit\" + 0.014*\"white\" + 0.009*\"black\" + 0.008*\"peopl\" + 0.008*\"stand\" + 0.007*\"like\" + 0.007*\"women\" + 0.007*\"wanna\" + 0.007*\"annoy\"\n",
            "Topic: 4 Word: 0.022*\"wrong\" + 0.016*\"peopl\" + 0.012*\"need\" + 0.012*\"think\" + 0.011*\"opinion\" + 0.011*\"immigr\" + 0.010*\"stop\" + 0.010*\"black\" + 0.009*\"asian\" + 0.008*\"like\"\n",
            "Topic: 5 Word: 0.015*\"black\" + 0.015*\"marri\" + 0.014*\"peopl\" + 0.013*\"women\" + 0.012*\"love\" + 0.010*\"race\" + 0.010*\"know\" + 0.010*\"ignor\" + 0.009*\"fuck\" + 0.009*\"white\"\n",
            "Topic: 6 Word: 0.024*\"women\" + 0.016*\"muslim\" + 0.012*\"jew\" + 0.011*\"hatr\" + 0.010*\"black\" + 0.010*\"fuck\" + 0.009*\"tell\" + 0.009*\"peopl\" + 0.007*\"terrorist\" + 0.007*\"like\"\n",
            "Topic: 7 Word: 0.011*\"suck\" + 0.010*\"peopl\" + 0.010*\"right\" + 0.010*\"fuck\" + 0.009*\"cunt\" + 0.008*\"associ\" + 0.008*\"love\" + 0.007*\"women\" + 0.007*\"work\" + 0.007*\"want\"\n",
            "Topic: 8 Word: 0.012*\"stupid\" + 0.012*\"gay\" + 0.011*\"peopl\" + 0.010*\"bitch\" + 0.009*\"women\" + 0.008*\"think\" + 0.008*\"muslim\" + 0.008*\"black\" + 0.008*\"indian\" + 0.008*\"rat\"\n",
            "Topic: 9 Word: 0.030*\"hate\" + 0.026*\"black\" + 0.020*\"fuck\" + 0.017*\"love\" + 0.017*\"peopl\" + 0.014*\"allow\" + 0.014*\"word\" + 0.012*\"like\" + 0.010*\"rude\" + 0.009*\"white\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjbTFxADK3sh",
        "outputId": "571cd813-94b2-401d-fa7c-0987fe8e5337"
      },
      "source": [
        "processed_docs[4310]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['black', 'watch', 'remind', 'black']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kakfk3fjLAK7",
        "outputId": "852edefe-15ba-444f-f109-f3ae3ef2693e"
      },
      "source": [
        "for index, score in sorted(lda_model[bow_corpus[4310]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Score: 0.8199864625930786\t \n",
            "Topic: 0.104*\"black\" + 0.059*\"white\" + 0.046*\"peopl\" + 0.020*\"tell\" + 0.017*\"mean\" + 0.015*\"start\" + 0.014*\"think\" + 0.013*\"matter\" + 0.011*\"attack\" + 0.010*\"woman\"\n",
            "\n",
            "Score: 0.02001127414405346\t \n",
            "Topic: 0.108*\"women\" + 0.035*\"fuck\" + 0.016*\"shit\" + 0.016*\"like\" + 0.016*\"know\" + 0.015*\"say\" + 0.010*\"point\" + 0.009*\"better\" + 0.009*\"look\" + 0.008*\"woman\"\n",
            "\n",
            "Score: 0.020001549273729324\t \n",
            "Topic: 0.064*\"like\" + 0.032*\"peopl\" + 0.028*\"black\" + 0.022*\"person\" + 0.020*\"hate\" + 0.018*\"actual\" + 0.013*\"world\" + 0.013*\"girl\" + 0.013*\"fuck\" + 0.011*\"talk\"\n",
            "\n",
            "Score: 0.020000282675027847\t \n",
            "Topic: 0.048*\"love\" + 0.045*\"peopl\" + 0.036*\"muslim\" + 0.029*\"fuck\" + 0.029*\"know\" + 0.023*\"hate\" + 0.018*\"tran\" + 0.018*\"marri\" + 0.016*\"get\" + 0.014*\"want\"\n",
            "\n",
            "Score: 0.020000215619802475\t \n",
            "Topic: 0.025*\"peopl\" + 0.019*\"best\" + 0.018*\"friend\" + 0.018*\"famili\" + 0.014*\"look\" + 0.013*\"love\" + 0.013*\"dont\" + 0.012*\"explain\" + 0.012*\"fuck\" + 0.012*\"want\"\n",
            "\n",
            "Score: 0.020000111311674118\t \n",
            "Topic: 0.039*\"fuck\" + 0.037*\"opinion\" + 0.031*\"want\" + 0.024*\"come\" + 0.022*\"think\" + 0.015*\"express\" + 0.015*\"idiot\" + 0.014*\"beauti\" + 0.014*\"allow\" + 0.013*\"inform\"\n",
            "\n",
            "Score: 0.020000047981739044\t \n",
            "Topic: 0.058*\"need\" + 0.038*\"stop\" + 0.035*\"peopl\" + 0.032*\"like\" + 0.022*\"word\" + 0.021*\"think\" + 0.021*\"fuck\" + 0.017*\"feel\" + 0.016*\"have\" + 0.015*\"call\"\n",
            "\n",
            "Score: 0.020000047981739044\t \n",
            "Topic: 0.025*\"good\" + 0.021*\"time\" + 0.019*\"thing\" + 0.017*\"go\" + 0.016*\"peopl\" + 0.014*\"work\" + 0.012*\"children\" + 0.011*\"talk\" + 0.010*\"school\" + 0.010*\"societi\"\n",
            "\n",
            "Score: 0.020000044256448746\t \n",
            "Topic: 0.046*\"peopl\" + 0.034*\"think\" + 0.025*\"wrong\" + 0.024*\"jew\" + 0.023*\"make\" + 0.019*\"kill\" + 0.013*\"disabl\" + 0.012*\"kid\" + 0.010*\"turn\" + 0.010*\"understand\"\n",
            "\n",
            "Score: 0.020000005140900612\t \n",
            "Topic: 0.022*\"countri\" + 0.019*\"year\" + 0.019*\"peopl\" + 0.015*\"allow\" + 0.014*\"leav\" + 0.013*\"issu\" + 0.012*\"chang\" + 0.012*\"life\" + 0.011*\"shit\" + 0.011*\"like\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWOYpygLLFFr",
        "outputId": "895cb19d-6721-4abf-ee38-7c1009b3b5b4"
      },
      "source": [
        "unseen_document = 'All Jews deserve to die'\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.47969916462898254\t Topic: 0.048*\"love\" + 0.045*\"peopl\" + 0.036*\"muslim\" + 0.029*\"fuck\" + 0.029*\"know\"\n",
            "Score: 0.2536140978336334\t Topic: 0.046*\"peopl\" + 0.034*\"think\" + 0.025*\"wrong\" + 0.024*\"jew\" + 0.023*\"make\"\n",
            "Score: 0.03334129974246025\t Topic: 0.039*\"fuck\" + 0.037*\"opinion\" + 0.031*\"want\" + 0.024*\"come\" + 0.022*\"think\"\n",
            "Score: 0.03333670273423195\t Topic: 0.064*\"like\" + 0.032*\"peopl\" + 0.028*\"black\" + 0.022*\"person\" + 0.020*\"hate\"\n",
            "Score: 0.03333630785346031\t Topic: 0.022*\"countri\" + 0.019*\"year\" + 0.019*\"peopl\" + 0.015*\"allow\" + 0.014*\"leav\"\n",
            "Score: 0.03333505615592003\t Topic: 0.025*\"good\" + 0.021*\"time\" + 0.019*\"thing\" + 0.017*\"go\" + 0.016*\"peopl\"\n",
            "Score: 0.03333483636379242\t Topic: 0.058*\"need\" + 0.038*\"stop\" + 0.035*\"peopl\" + 0.032*\"like\" + 0.022*\"word\"\n",
            "Score: 0.03333430737257004\t Topic: 0.108*\"women\" + 0.035*\"fuck\" + 0.016*\"shit\" + 0.016*\"like\" + 0.016*\"know\"\n",
            "Score: 0.033334117382764816\t Topic: 0.104*\"black\" + 0.059*\"white\" + 0.046*\"peopl\" + 0.020*\"tell\" + 0.017*\"mean\"\n",
            "Score: 0.033334068953990936\t Topic: 0.025*\"peopl\" + 0.019*\"best\" + 0.018*\"friend\" + 0.018*\"famili\" + 0.014*\"look\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CQoLWbELRB9",
        "outputId": "a700acc8-0483-42a7-93cb-8ad7ea358de5"
      },
      "source": [
        "unseen_document = 'So interesting to see progressive Democrat congresswomen, who originally came from countries whose governments are a complete and total catastrophe, the worst, most corrupt and inept anywhere in the world'\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.6560420989990234\t Topic: 0.022*\"countri\" + 0.019*\"year\" + 0.019*\"peopl\" + 0.015*\"allow\" + 0.014*\"leav\"\n",
            "Score: 0.17694002389907837\t Topic: 0.048*\"love\" + 0.045*\"peopl\" + 0.036*\"muslim\" + 0.029*\"fuck\" + 0.029*\"know\"\n",
            "Score: 0.113163061439991\t Topic: 0.025*\"good\" + 0.021*\"time\" + 0.019*\"thing\" + 0.017*\"go\" + 0.016*\"peopl\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw1wNMLzBw2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c06e4657-6c64-4498-a059-8c46050bd722"
      },
      "source": [
        "from gensim.models import CoherenceModel\n",
        "# Compute Coherence Score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score:  0.2846339425358257\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4Kbw2nAalZE",
        "outputId": "d712382b-634f-4135-bf88-d8d4d4fab7bb"
      },
      "source": [
        "coherence_model_lda_tfidf = CoherenceModel(model=lda_model_tfidf, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda_tfidf = coherence_model_lda_tfidf.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda_tfidf)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Coherence Score:  0.26267082083738985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SewpAMMscI4E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}